# Multi-stage Dockerfile for XTTS API Server with GPU support
# Optimized for production deployment on AWS G4 instances

# Stage 1: Builder
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 AS builder

# Install Python and build dependencies
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3.11-venv \
    python3-pip \
    git \
    wget \
    build-essential \
    portaudio19-dev \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# Create virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Upgrade pip
RUN pip install --upgrade pip setuptools wheel

# Copy requirements first for better caching
COPY requirements.txt /tmp/requirements.txt

# Install PyTorch with CUDA 11.8 support
RUN pip install torch==2.1.1+cu118 torchaudio==2.1.1+cu118 --index-url https://download.pytorch.org/whl/cu118

# Install other dependencies
RUN pip install -r /tmp/requirements.txt

# Install DeepSpeed for faster inference
RUN pip install deepspeed

# Stage 2: Runtime
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Install runtime dependencies only
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-distutils \
    portaudio19-dev \
    libsndfile1 \
    ffmpeg \
    espeak-ng \
    git \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.11 as default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# Copy virtual environment from builder
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Set working directory
WORKDIR /app

# Copy application code
COPY xtts_api_server/ ./xtts_api_server/
COPY example/ ./example/

# Create necessary directories with proper permissions
RUN mkdir -p /app/speakers /app/output /app/xtts_models && \
    chmod -R 777 /app/speakers /app/output /app/xtts_models

# Pre-download the default model to speed up first run
RUN python -c "from TTS.api import TTS; import torch; \
    if torch.cuda.is_available(): \
        print('CUDA available, downloading model...'); \
        tts = TTS('tts_models/multilingual/multi-dataset/xtts_v2').to('cuda')"

# Environment variables for GPU support
ENV CUDA_VISIBLE_DEVICES=0
ENV TORCH_CUDA_ARCH_LIST="7.5;8.0;8.6"
ENV DEVICE=cuda
ENV MODEL_SOURCE=local
ENV MODEL_VERSION=v2.0.2
ENV USE_CACHE=true
ENV DEEPSPEED=false
ENV LOWVRAM_MODE=false

# Expose the API port
EXPOSE 8020

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8020/languages').raise_for_status()" || exit 1

# Run the server
CMD ["python", "-m", "xtts_api_server", \
     "--host", "0.0.0.0", \
     "--port", "8020", \
     "--device", "cuda", \
     "--use-cache"]